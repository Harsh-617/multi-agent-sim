# Multi-Agent Simulation Platform

![Status](https://img.shields.io/badge/status-WIP%20%E2%80%93%20stable-yellow)
![Python](https://img.shields.io/badge/python-3.11-blue)
![License](https://img.shields.io/badge/license-MIT-green)
![Tests](https://img.shields.io/badge/tests-passing-brightgreen)

A research platform for studying emergent strategy and cooperation in multi-agent environments. Agents with heterogeneous policies interact inside a configurable shared-resource environment; their behavior is recorded, evaluated across stress-tested environment variants, and visualized through a full-stack web dashboard. Training is done via PPO with league-based evolutionary self-play, producing a lineage of competitive policy snapshots with Elo ratings and robustness profiles.

The system is designed around a single, well-specified environment archetype â€” the **Mixed** environment, which models a shared resource pool where agents choose to cooperate, extract, or defend. The configuration surface is deep: five orthogonal behavioral layers control information asymmetry, temporal memory depth, reputation dynamics, incentive regime, and observation uncertainty. Everything downstream â€” training, evaluation, robustness sweeps, the backend API, and the dashboard â€” operates on this single source of truth.

---

## System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        multi-agent-sim Â· V1 Architecture                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Config       â”‚     â”‚                 Simulation Engine                   â”‚
  â”‚  (Pydantic)   â”‚â”€â”€â”€â”€â–¶â”‚                                                    â”‚
  â”‚               â”‚     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚ Â· 5 layers    â”‚     â”‚  â”‚  MixedEnvironment â”‚â—€â”€â”€â”‚        Agents          â”‚ â”‚
  â”‚ Â· RewardWeights     â”‚  â”‚                  â”‚   â”‚  Â· Random              â”‚ â”‚
  â”‚ Â· PopulationCfgâ”‚    â”‚  â”‚  4 action types  â”‚   â”‚  Â· AlwaysCooperate     â”‚ â”‚
  â”‚ Â· Validation  â”‚     â”‚  â”‚  3-component rwd â”‚   â”‚  Â· TitForTat           â”‚ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”‚  3 terminations  â”‚   â”‚  Â· PPOShared (infer)   â”‚ â”‚
                        â”‚  â”‚  PettingZoo wrap â”‚   â”‚  Â· LeagueSnapshot      â”‚ â”‚
                        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ rollout data
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚                 PPO Training                        â”‚
                        â”‚  CleanRL-style Â· Shared trunk (64â†’64 Tanh)          â”‚
                        â”‚  Categorical action-type head + Beta amount head     â”‚
                        â”‚  GAE advantage Â· Clipped surrogate Â· Ent. bonus      â”‚
                        â”‚  League self-play mode + periodic snapshotting       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ policy.pt + metadata.json
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚               League Registry                       â”‚
                        â”‚  Filesystem snapshots Â· Parentâ€“child lineage        â”‚
                        â”‚  Elo ratings (K=32) Â· Weighted opponent sampler     â”‚
                        â”‚  Recent-vs-old bias Â· Max-member trimming           â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ champion + member pool
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚             Robustness Evaluation                   â”‚
                        â”‚  10 environment variants (info, noise, incentive,   â”‚
                        â”‚  scarcity, population, combined stress)             â”‚
                        â”‚  Score = 0.7 Ã— mean_reward + 0.3 Ã— worst_case      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ JSON + Markdown reports
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚              FastAPI Backend                        â”‚
                        â”‚  7 routers Â· WebSocket per-step metrics stream      â”‚
                        â”‚  Async experiment runner Â· Run state manager        â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚ REST + WebSocket
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚            Next.js 14 Dashboard                     â”‚
                        â”‚  League evolution graph (SVG) Â· Robustness heatmap  â”‚
                        â”‚  Live metrics chart Â· Champion benchmark             â”‚
                        â”‚  Run history Â· Report browser                       â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Runtime Data Flow

```
  User saves config JSON
          â”‚
          â–¼
  POST /api/runs/start  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                                                â”‚
          â–¼                                                                â”‚
  ExperimentRunner (async task)                                            â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
  â”‚  env.reset(seed)  â”€â”€â–¶  initial observations                     â”‚      â”‚
  â”‚  loop each step:                                                â”‚      â”‚
  â”‚    agent.act(obs)   â”€â”€â–¶  action                                 â”‚      â”‚
  â”‚    env.step(actions) â”€â”€â–¶  rewards, next obs, done, info        â”‚      â”‚
  â”‚    metrics.collect_step()                                       â”‚      â”‚
  â”‚    mgr.broadcast({type:"step", metrics, events})  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶  ws://â€¦/ws/metrics
  â”‚  on done: write run artifacts                                   â”‚      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
          â”‚                                                                â”‚
          â–¼                                                                â–¼
  storage/runs/{run_id}/                                    /run/[id] live chart
  â”œâ”€â”€ config.json
  â”œâ”€â”€ metrics.jsonl
  â”œâ”€â”€ events.jsonl
  â””â”€â”€ episode_summary.json
```

---

## Feature Checklist

### Simulation Engine
- âœ… `BaseEnvironment` abstract contract (lifecycle, obs/action specs, termination)
- âœ… `MixedEnvironment` â€” shared resource pool with cooperative/competitive/defensive dynamics
- âœ… Four action types: `COOPERATE`, `EXTRACT`, `DEFEND`, `CONDITIONAL`
- âœ… Three-component reward: individual gain + group pool health + relational reputation
- âœ… Three termination conditions: max steps, shared-pool collapse, all-agents-eliminated
- âœ… Five behavioral config layers (see [Configuration](#configuration))
- âœ… PettingZoo `ParallelEnv` wrapper for standard RL library compatibility
- âœ… Fully deterministic seeding via `derive_seed(base, offset)` propagation
- ğŸš§ Additional environment archetypes (cooperative-only, competitive, hierarchical, negotiation, market, emergency, deceptive)
- ğŸš§ Multi-archetype composition in a single session

### Agents & Policies
- âœ… `BaseAgent` abstract interface
- âœ… Rule-based baselines: `RandomAgent`, `AlwaysCooperate`, `AlwaysExtract`, `TitForTat`
- âœ… `PPOSharedAgent` â€” lazy-loaded inference wrapper, deterministic and stochastic modes
- âœ… `LeagueSnapshotAgent` â€” loads a specific league checkpoint by member ID
- âœ… Observation flattening: step, pool, resources, peer cooperation scores, action-history window
- ğŸš§ Recurrent policies (LSTM)
- ğŸš§ Population-based training
- ğŸš§ Imitation learning from recorded trajectories

### Training
- âœ… Shared-policy PPO (CleanRL-style, homogeneous population)
- âœ… Actor-critic network: shared trunk â†’ Categorical action-type head + Beta amount head + value head
- âœ… Rollout buffer with GAE advantage estimation
- âœ… Clipped surrogate objective, value loss (MSE), entropy bonus, gradient clipping
- âœ… League self-play mode with configurable opponent sampling weights
- âœ… Periodic automatic snapshotting to league registry during training
- âœ… TensorBoard logging (optional)
- âœ… Artifact export: `policy.pt` + `metadata.json` (hyperparams, config hash, obs layout)
- ğŸš§ Training runs managed through the backend API / UI

### League System
- âœ… `LeagueRegistry` â€” filesystem-backed snapshot store with monotonic member IDs
- âœ… Parentâ€“child lineage tracking across training generations
- âœ… Elo ratings with pairwise evaluation (K=32)
- âœ… `OpponentSampler` â€” weighted sampling across league members, baselines, and fixed policies
- âœ… Configurable recent-vs-old bias; registry auto-trims at max member count
- âœ… Population evaluation runner

### Evaluation & Robustness
- âœ… Cross-seed policy evaluator: N policies Ã— M seeds, mean/std returns, collapse rate, episode length
- âœ… 10 default robustness sweep variants (information, noise, incentive, scarcity, population, combined)
- âœ… Robustness score: `0.7 Ã— overall_mean + 0.3 Ã— worst_case_reward`
- âœ… Strategy feature extraction and k-means clustering (NumPy-only, deterministic)
- âœ… Report generation: JSON + Markdown, named by type/config/timestamp
- âœ… Full pipeline orchestrator: training â†’ league creation â†’ evaluation â†’ report

### Backend API
- âœ… FastAPI application with 7 routers
- âœ… Run lifecycle: start, stop (graceful), status
- âœ… WebSocket per-step metrics stream (`ws://â€¦/ws/metrics`)
- âœ… Config CRUD, run history, league endpoints, report endpoints
- âœ… Champion identification, champion benchmark vs. baselines, champion robustness sweep
- âœ… League evolution endpoint: lineage + strategy labels + timeline
- ğŸš§ Concurrent multi-run support

### Frontend Dashboard
- âœ… Live run view with real-time metrics chart over WebSocket
- âœ… League evolution page: SVG lineage graph with parentâ€“child edges, color by strategy label
- âœ… Robustness heatmap, scatter plot, and summary table
- âœ… Champion benchmark bar chart vs. baselines
- âœ… Run history browser and report browser
- âœ… Typed API client (`lib/api.ts`)
- ğŸš§ Interactive step-through replay of completed runs
- ğŸš§ User-defined configuration templates from the UI
- ğŸš§ Agent-level metric breakdowns and action-distribution histograms

### Infrastructure & Testing
- âœ… 22 unit test modules covering all major simulation and league components
- âœ… 4 integration test modules covering API contracts (using `httpx`)
- âœ… `environment.yml` for reproducible Conda setup (Python 3.11 + Node 18)
- âœ… `pyproject.toml` with optional `[dev]` and `[training]` dependency groups
- ğŸš§ Authentication and multi-user support
- ğŸš§ Packaged deployment (Docker)

---

## Tech Stack

| Layer | Technology |
|---|---|
| Simulation core | Python 3.11, NumPy |
| Configuration | Pydantic V2 |
| RL training | PyTorch, CleanRL-style PPO |
| RL compatibility | PettingZoo `ParallelEnv` |
| Backend | FastAPI, Uvicorn, WebSocket (`websockets`) |
| Frontend | Next.js 14, React, Tailwind CSS |
| Storage | Filesystem JSON / JSONL |
| Testing | pytest, httpx |
| Optional logging | TensorBoard |

---

## Repository Layout

```
multi-agent-sim/
â”œâ”€â”€ simulation/                  # Core Python engine
â”‚   â”œâ”€â”€ core/                    # BaseEnvironment, types, seeding
â”‚   â”œâ”€â”€ envs/
â”‚   â”‚   â””â”€â”€ mixed/               # MixedEnvironment (V1 archetype)
â”‚   â”‚       â”œâ”€â”€ env.py           # Main environment class
â”‚   â”‚       â”œâ”€â”€ actions.py       # ActionType enum + Action dataclass
â”‚   â”‚       â”œâ”€â”€ state.py         # AgentState, RelationalState, GlobalState
â”‚   â”‚       â”œâ”€â”€ rewards.py       # Three-component reward computation
â”‚   â”‚       â”œâ”€â”€ transition.py    # Action resolution + state mutation
â”‚   â”‚       â””â”€â”€ termination.py   # Episode termination checks
â”‚   â”œâ”€â”€ adapters/                # PettingZoo parallel env wrapper
â”‚   â”œâ”€â”€ agents/                  # BaseAgent + baselines + PPO + league agents
â”‚   â”œâ”€â”€ config/                  # MixedEnvironmentConfig schema + defaults
â”‚   â”œâ”€â”€ training/                # PPO training loop (standard + league self-play)
â”‚   â”œâ”€â”€ league/                  # Registry, ratings, sampler, population eval
â”‚   â”œâ”€â”€ evaluation/              # Cross-seed evaluator, robustness sweeps, reporting
â”‚   â”œâ”€â”€ analysis/                # Strategy feature extraction + k-means clustering
â”‚   â”œâ”€â”€ metrics/                 # MetricsCollector, event definitions
â”‚   â”œâ”€â”€ runner/                  # RunLogger (artifact persistence)
â”‚   â””â”€â”€ pipeline/                # End-to-end pipeline orchestrator
â”‚
â”œâ”€â”€ backend/                     # FastAPI server
â”‚   â”œâ”€â”€ api/                     # Route modules
â”‚   â”‚   â”œâ”€â”€ routes_config.py
â”‚   â”‚   â”œâ”€â”€ routes_experiment.py
â”‚   â”‚   â”œâ”€â”€ routes_history.py
â”‚   â”‚   â”œâ”€â”€ routes_league.py
â”‚   â”‚   â”œâ”€â”€ routes_pipeline.py
â”‚   â”‚   â”œâ”€â”€ routes_reports.py
â”‚   â”‚   â””â”€â”€ ws_metrics.py        # WebSocket endpoint
â”‚   â”œâ”€â”€ runner/                  # ExperimentRunner (async) + RunManager (state)
â”‚   â”œâ”€â”€ schemas/                 # Pydantic request/response models
â”‚   â””â”€â”€ main.py                  # App assembly + router registration
â”‚
â”œâ”€â”€ frontend/                    # Next.js 14 dashboard
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ app/                 # Pages: home, /run/[id], /runs, /league, /reports/[id]
â”‚       â”œâ”€â”€ components/          # React components
â”‚       â”‚   â”œâ”€â”€ LeagueEvolution.tsx    # SVG lineage graph + champion timeline
â”‚       â”‚   â”œâ”€â”€ ChampionRobustness.tsx # Robustness sweep form
â”‚       â”‚   â”œâ”€â”€ ChampionBenchmark.tsx  # Baseline comparison bar chart
â”‚       â”‚   â”œâ”€â”€ MetricsChart.tsx       # Real-time step chart
â”‚       â”‚   â”œâ”€â”€ RobustHeatmap.tsx      # Policy Ã— sweep heatmap
â”‚       â”‚   â”œâ”€â”€ RobustScatter.tsx      # Mean vs. worst-case scatter
â”‚       â”‚   â””â”€â”€ RobustSummaryTable.tsx # Aggregated stats table
â”‚       â””â”€â”€ lib/api.ts           # Typed REST + WebSocket client
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                    # 22 test modules (agents, env, league, training, â€¦)
â”‚   â””â”€â”€ integration/             # 4 API contract test modules
â”‚
â”œâ”€â”€ storage/                     # Runtime artifacts (gitignored, .gitkeep present)
â”‚   â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ runs/
â”‚   â”œâ”€â”€ agents/
â”‚   â””â”€â”€ reports/
â”‚
â”œâ”€â”€ design/                      # Architecture docs, roadmap, planning notes
â”‚                                # (not implemented code â€” treat as design artifacts)
â”œâ”€â”€ environment.yml
â””â”€â”€ pyproject.toml
```

---

## Setup

### Option A â€” Conda (recommended)

```bash
git clone https://github.com/<your-username>/multi-agent-sim.git
cd multi-agent-sim

conda env create -f environment.yml
conda activate multi-agent-sim

cd frontend && npm install && cd ..
```

The Conda environment installs Python 3.11, Node 18, and the Python package in editable mode with both `[dev]` and `[training]` extras.

### Option B â€” pip + venv

```bash
git clone https://github.com/<your-username>/multi-agent-sim.git
cd multi-agent-sim

python -m venv .venv
# Linux/macOS:
source .venv/bin/activate
# Windows:
.venv\Scripts\activate

pip install -e ".[dev]"         # runtime + test deps
pip install -e ".[training]"    # adds PyTorch, tqdm, TensorBoard

cd frontend && npm install && cd ..
```

> PyTorch and TensorBoard are **not** installed by default. The backend server and all evaluation code run without them. Only training requires `[training]`.

---

## One-Command Demo

The following sequence runs the full pipeline â€” training, league creation, robustness evaluation, and report generation â€” then opens the dashboard to explore the results.

**Step 1 â€” Run the end-to-end pipeline**

```bash
python -m simulation.pipeline.pipeline_run
```

This executes in order:
1. PPO training on `MixedEnvironment` with league self-play
2. Periodic policy snapshots saved to `storage/agents/league/`
3. Elo ratings computed across all league members
4. Robustness sweep: champion policy evaluated across 10 environment variants
5. Report written to `storage/reports/`

**Step 2 â€” Start the backend**

```bash
uvicorn backend.main:app --port 8000
```

API docs available at `http://localhost:8000/docs`.

**Step 3 â€” Start the frontend**

```bash
cd frontend && npm run dev
```

Dashboard available at `http://localhost:3000`.

**Step 4 â€” Explore the results**

| UI Page | URL | What to look for |
|---|---|---|
| League Evolution | `localhost:3000/league` | SVG lineage graph, Elo ratings, champion timeline |
| Robustness Report | `localhost:3000/reports` | Heatmap of policy Ã— sweep, scatter, worst-case analysis |
| Run History | `localhost:3000/runs` | Episode metrics from training runs |

**Where artifacts are written**

```
storage/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ppo_shared/
â”‚   â”‚   â”œâ”€â”€ policy.pt            # final trained weights
â”‚   â”‚   â””â”€â”€ metadata.json        # hyperparams, config_hash, obs_dim
â”‚   â””â”€â”€ league/
â”‚       â”œâ”€â”€ league_000001/       # earliest snapshot
â”‚       â”‚   â”œâ”€â”€ policy.pt
â”‚       â”‚   â””â”€â”€ metadata.json    # parent_id, created_at, notes, elo history
â”‚       â””â”€â”€ league_000NNN/       # most recent snapshot
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ robust_{config_hash}_{timestamp}/
â”‚       â”œâ”€â”€ report.json          # full structured results
â”‚       â””â”€â”€ report.md            # human-readable summary
â””â”€â”€ runs/
    â””â”€â”€ {run_id}/
        â”œâ”€â”€ config.json
        â”œâ”€â”€ metrics.jsonl        # per-step metrics stream
        â”œâ”€â”€ events.jsonl         # semantic events (collapse, elimination)
        â””â”€â”€ episode_summary.json
```

---

## Configuration

All simulation behavior is controlled by a single `MixedEnvironmentConfig` (Pydantic V2). The config is the canonical unit of reproducibility â€” its SHA-256 hash is embedded in every trained artifact.

### Behavioral Layers

| Layer | Parameter | Range | Effect |
|---|---|---|---|
| Information asymmetry | `information_asymmetry` | 0.0 â€“ 1.0 | Fraction of observation masked per agent |
| Temporal memory | `temporal_memory_depth` | 1 â€“ 50 | Observation history window depth (steps) |
| Reputation | `reputation_sensitivity` | 0.0 â€“ 1.0 | EMA weight on cooperation history |
| Incentive regime | `incentive_softness` | 0.0 â€“ 1.0 | 0 = hard bans, 1 = soft penalties only |
| Uncertainty | `uncertainty_intensity` | 0.0 â€“ 0.5 | Gaussian noise magnitude on observations |

### Reward Weights

```
reward = individual_weight Ã— individual_component
       + group_weight     Ã— group_component
       + relational_weight Ã— relational_component
       Ã— (1 + penalty_scaling Ã— active_penalties)
```

### Population Defaults

| Parameter | Default |
|---|---|
| `num_agents` | 5 |
| `max_steps` | 200 |
| `initial_shared_pool` | 100.0 |
| `initial_resources` | 20.0 (per agent) |
| `collapse_threshold` | â‰¤ initial_shared_pool (validated) |

Configs are saved as JSON to `storage/configs/` and referenced by ID in API calls and run logs.

---

## Training

```bash
python -m simulation.training.ppo_shared
```

### Network Architecture

```
Observation (flattened dict)
        â”‚
        â–¼
Linear(obs_dim â†’ 64) â†’ Tanh â†’ Linear(64 â†’ 64) â†’ Tanh   (shared trunk)
        â”‚                              â”‚                         â”‚
        â–¼                              â–¼                         â–¼
Linear(64 â†’ 4)              Linear(64 â†’ 1) Ã— 2         Linear(64 â†’ 1)
Categorical dist            softplus + 1 â†’ Î±, Î²          state value V(s)
(action type)               Beta dist (amount âˆˆ [0,1])
```

### PPO Hyperparameters (defaults)

| Parameter | Value |
|---|---|
| `total_timesteps` | 50,000 |
| `rollout_steps` | 256 |
| `ppo_epochs` | 4 |
| `num_minibatches` | 4 |
| `learning_rate` | 3e-4 |
| `gamma` | 0.99 |
| `gae_lambda` | 0.95 |
| `clip_eps` | 0.2 |
| `vf_coef` | 0.5 |
| `ent_coef` | 0.01 |

### League Self-Play

Training can be run against a dynamically sampled opponent pool:

- **Baselines** â€” random, always-cooperate, always-extract, tit-for-tat
- **League members** â€” past policy snapshots (weighted toward recent)
- **Fixed policies** â€” optional pretrained policy (e.g., `ppo_shared`)

Sampling weights and the recent-vs-old bias are configurable. Every N timesteps, the current policy is snapshotted and added to the league registry. The registry auto-trims at a configured `max_members` limit, preserving the lineage record in metadata.

---

## Evaluation & Robustness

### Cross-Seed Evaluation

```python
evaluate_policies(
    config,
    policy_specs,        # list of {name, source, league_member_id}
    seeds=[42, 43, 44],
    episodes_per_seed=2
) -> list[PolicyResult]
```

Each `PolicyResult` aggregates: `mean_total_reward Â± std`, `mean_final_shared_pool Â± std`, `collapse_rate`, `mean_episode_length`, and a per-seed breakdown for reproducibility verification.

### Robustness Sweeps

Ten environment variants test generalization beyond the training distribution:

| Sweep | What changes |
|---|---|
| `obs_ia_0.0` | No information asymmetry |
| `obs_ia_0.6` | High information asymmetry |
| `uncertainty_0.0` | No observation noise |
| `uncertainty_0.3` | High observation noise |
| `incentive_soft_0.2` | Hard penalty regime |
| `incentive_soft_0.8` | Soft penalty regime |
| `pool_scarce` | Shared pool Ã— 0.5 |
| `pool_abundant` | Shared pool Ã— 1.5 |
| `pop_10` | 10 agents instead of 5 |
| `combined_hard` | High asymmetry + scarcity + noise |

**Robustness score:**

```
robustness_score = 0.7 Ã— overall_mean_reward + 0.3 Ã— worst_case_reward
```

This explicitly penalizes brittle policies that perform well on average but collapse under specific conditions.

---

## Backend API

| Router | Base Path | Key Endpoints |
|---|---|---|
| Config | `/api/configs` | `GET /` list, `GET /{id}` retrieve |
| Experiment | `/api/runs` | `POST /start`, `POST /stop`, `GET /status` |
| History | `/api/history` | `GET /` past runs list |
| League | `/api/league` | `GET /members`, `GET /champion`, `GET /lineage`, `GET /evolution` |
| League (actions) | `/api/league` | `POST /ratings/recompute`, `POST /champion/benchmark`, `POST /champion/robustness` |
| Reports | `/api/reports` | `GET /` list, `GET /{id}` full report, `GET /{id}/strategies` |
| WebSocket | `ws://â€¦/ws/metrics` | Per-step metrics stream for live runs |

Interactive API documentation: `http://localhost:8000/docs`

---

## Frontend

| Page | Route | Description |
|---|---|---|
| Home | `/` | Config list, navigation |
| Live Run | `/run/[run_id]` | Real-time metrics chart via WebSocket, stop control |
| Run History | `/runs` | Completed run index |
| League | `/league` | Evolution graph + champion benchmark |
| Reports | `/reports` | Report index |
| Report Detail | `/reports/[report_id]` | Robustness heatmap, scatter plot, summary table |

### Key Components

**`LeagueEvolution.tsx`** â€” Renders an SVG forest layout of the policy lineage tree. Nodes are sized by Elo rating and color-coded by strategy label (Champion = gold, Competitive = blue, Developing = gray). Clicking a node opens a metadata detail panel. A secondary panel renders a chronological champion timeline.

**`ChampionRobustness.tsx`** â€” Form for triggering a new robustness sweep against the current champion. Accepts config selection, seeds, episodes per seed, and optional sweep limits. On completion, redirects to the generated report.

**`RobustHeatmap.tsx`** â€” Heatmap with policies on rows and environment sweep variants on columns. Cell color encodes mean reward. Provides immediate visual identification of policy weaknesses.

---

## Artifact Layout

```
storage/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ {config_id}.json              # Full MixedEnvironmentConfig snapshot
â”‚
â”œâ”€â”€ runs/
â”‚   â””â”€â”€ {run_id}/
â”‚       â”œâ”€â”€ config.json               # Config used for this run
â”‚       â”œâ”€â”€ metrics.jsonl             # Per-step: step, agent_id, reward, action, pool, resources
â”‚       â”œâ”€â”€ events.jsonl              # Sparse semantic events: COLLAPSE_DETECTED, AGENT_DEACTIVATED
â”‚       â””â”€â”€ episode_summary.json     # Aggregated: length, termination_reason, total rewards
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ppo_shared/
â”‚   â”‚   â”œâ”€â”€ policy.pt                 # Trained weights (state_dict)
â”‚   â”‚   â””â”€â”€ metadata.json            # hyperparams, config_hash, obs_dim, timestamp
â”‚   â””â”€â”€ league/
â”‚       â””â”€â”€ league_{N:06d}/          # Monotonically numbered
â”‚           â”œâ”€â”€ policy.pt
â”‚           â””â”€â”€ metadata.json        # + member_id, parent_id, created_at, notes
â”‚
â””â”€â”€ reports/
    â””â”€â”€ robust_{config_hash}_{timestamp}/
        â”œâ”€â”€ report.json              # Structured: metadata, per-sweep results, summary
        â””â”€â”€ report.md                # Human-readable markdown
```

---

## Reproducibility

All stochastic components are seeded from a single integer root seed:

- **Environment resets** â€” `env.reset(seed=seed)` accepts an explicit seed.
- **Agent initialization** â€” `agent.reset(agent_id, seed=seed)` called at episode start.
- **Seed propagation** â€” `derive_seed(base_seed, offset)` generates deterministic per-agent seeds from the root, eliminating seed-collision artifacts in multi-agent settings.
- **NumPy RNG isolation** â€” `make_rng(seed)` creates an independent `numpy.random.Generator` per component.
- **Config hash** â€” SHA-256 of the serialized `MixedEnvironmentConfig` is written into every `metadata.json`. Given the same config hash and seed, a training run or evaluation is exactly reproducible.
- **Strategy clustering** â€” k-means runs with a fixed seed, producing deterministic cluster assignments.

To reproduce any result: locate the `config_hash` in the artifact's `metadata.json`, retrieve the matching config from `storage/configs/`, and re-run with the same seed.

---

## Testing

```bash
# Full suite
pytest tests/

# Unit tests only (no running server required)
pytest tests/unit/ -v

# Integration tests (requires backend running on port 8000)
pytest tests/integration/ -v

# Coverage report
pytest tests/ --cov=simulation --cov=backend --cov-report=term-missing
```

**Unit coverage** (22 modules): `mixed_env`, `config_validation`, `agents`, `ppo_training`, `league_registry`, `league_ratings`, `league_sampling`, `league_selfplay`, `evaluation`, `robustness`, `reporting`, `metrics_collector`, `run_logger`, `pipeline_run`, `strategy_analysis`, PettingZoo adapter, seeding utilities, and more.

**Integration coverage** (4 modules): experiment lifecycle, run history, league endpoints, report endpoints â€” all tested via `httpx` against the live FastAPI application.

---

## Screenshots *(Coming Soon)*

- **League evolution graph** â€” SVG lineage tree with Elo-sized nodes and strategy color coding
- **Robustness heatmap** â€” policy Ã— environment variant grid showing performance profile
- **Strategy evolution timeline** â€” champion lineage over training generations
- **Live metrics chart** â€” real-time reward, shared pool, and cooperation ratio during a run

---

## Project Status

This is a **solo research project** under active development. The simulation engine, training pipeline, league system, evaluation framework, backend API, and frontend dashboard are all implemented and tested. The codebase is stable and all tests pass.

All functionality described outside the [`design/`](design/) directory is fully implemented and runnable. Files under `design/` contain architectural reasoning, future plans, and exploratory ideas â€” they should not be interpreted as implemented features.

Planned extensions include additional environment archetypes, expanded strategy analysis, and user-defined configuration templates. These are tracked in the design documents and will be implemented incrementally.

**External contributions are not being accepted at this time.**

---

## License

MIT â€” see [LICENSE](LICENSE).
